{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzkU0YmNk4ljJXZVLfmiJh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install tweepy"
      ],
      "metadata": {
        "id": "vTnFtjqshlH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tweepy\n",
        "# import configparser\n",
        "# import pandas as pd\n"
      ],
      "metadata": {
        "id": "EzvPhiIPhvWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # read configs\n",
        "# # config = configparser.ConfigParser()\n",
        "# # config.read('config.ini')\n",
        "\n",
        "# api_key = 'sZ41ftVMY7zqoSVdcp4fpodZz'\n",
        "# api_key_secret = '1X043vSW8FV1hedSENFbgaaryENa8dh9A9LFg5sdCu7AuZMnX7'\n",
        "\n",
        "# access_token = '1268928347494420480-rhplk9UBYdbWXz0KERZCsDlbJv0tLr'\n",
        "# access_token_secret = 'EuY9IdIPxxL3038W7Cf3pMCbpXVWvq1R2CmcnddYRZcYa'\n",
        "\n",
        "# # authentication\n",
        "# auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "# auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# api = tweepy.API(auth)\n",
        "\n",
        "# public_tweets = api.home_timeline()\n",
        "\n",
        "# print(public_tweets)\n",
        "\n",
        "# # create dataframe\n",
        "# # columns = ['Time', 'User', 'Tweet']\n",
        "# # data = []\n",
        "# # for tweet in public_tweets:\n",
        "# #     data.append([tweet.created_at, tweet.user.screen_name, tweet.text])\n",
        "\n",
        "# # df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# # df.to_csv('tweets.csv')"
      ],
      "metadata": {
        "id": "JVBdM0kbolez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape"
      ],
      "metadata": {
        "id": "tRM_KmvApWMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNDFJIHahhJE"
      },
      "outputs": [],
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "zzz=[]\n",
        "# query = \"(from:elonmusk) until:2020-01-01 since:2010-01-01\"\n",
        "\n",
        "# query = \"indian stocks\"\n",
        "# tweets = []\n",
        "# limit = 5\n",
        "\n",
        "\n",
        "# for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "#     # print(tweet.content)\n",
        "#     # break\n",
        "#     if len(tweets) == limit:\n",
        "#         break\n",
        "#     else:\n",
        "#         tweets.append([tweet.date, tweet.content])\n",
        "        \n",
        "# df = pd.DataFrame(tweets, columns=['Date', 'Tweet'])\n",
        "# print(df)\n",
        "\n",
        "# to save to csv\n",
        "# df.to_csv('tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df"
      ],
      "metadata": {
        "id": "mwtjzJ2al7Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "#     if len(tweets) == limit:\n",
        "#         break\n",
        "#     else:\n",
        "#         tweets.append([tweet.content])\n"
      ],
      "metadata": {
        "id": "Jn2kNMtCernC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tweets"
      ],
      "metadata": {
        "id": "f9Fj97HbetbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df"
      ],
      "metadata": {
        "id": "AuZE4fG3xKBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "lVtZLIvdt-gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request\n",
        "\n",
        "# Preprocess text (username and link placeholders)\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        " \n",
        " \n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "# Tasks:\n",
        "# emoji, emotion, hate, irony, offensive, sentiment\n",
        "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
        "\n",
        "task='sentiment'\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "\n",
        "# download label mapping\n",
        "labels=[]\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "    csvreader = csv.reader(html, delimiter='\\t')\n",
        "labels = [row[1] for row in csvreader if len(row) > 1]\n",
        "\n",
        "\n",
        "# PT\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "model.save_pretrained(MODEL)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z82ukJ1-qPxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def sentm(text):\n",
        "\n",
        "#   # text = \"\"\"@Curious_Indian_ But after selling we'll have to research again to buy new stocks, that is also time consuming\"\"\"\n",
        "#   respp=[]\n",
        "#   text = preprocess(text)\n",
        "#   encoded_input = tokenizer(text, return_tensors='pt')\n",
        "#   output = model(**encoded_input)\n",
        "#   scores = output[0][0].detach().numpy()\n",
        "#   scores = softmax(scores)\n",
        "\n",
        "#   ranking = np.argsort(scores)\n",
        "#   ranking = ranking[::-1]\n",
        "#   for i in range(scores.shape[0]):\n",
        "#       l = labels[ranking[i]]\n",
        "#       s = scores[ranking[i]]\n",
        "#       print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
        "#       respp.append(np.round(float(s), 4))\n",
        "\n",
        "#   # print(f\"{0}) {labels[ranking[0]]} {np.round(float(scores[ranking[0]]), 4)}\")\n",
        "#   # resp=np.round(float(scores[ranking[0]]), 4)\n",
        "#   return respp\n",
        "\n",
        "\n",
        "# # # TF\n",
        "# # model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "# # model.save_pretrained(MODEL)\n",
        "\n",
        "# # text = \"Good night ðŸ˜Š\"\n",
        "# # encoded_input = tokenizer(text, return_tensors='tf')\n",
        "# # output = model(encoded_input)\n",
        "# # scores = output[0][0].numpy()\n",
        "# # scores = softmax(scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "7QLSRrlxvNXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentmm(text):\n",
        "    # text = \"\"\"you are a  boy\"\"\"\n",
        "    respp=[]\n",
        "    text = preprocess(text)\n",
        "    encoded_input = tokenizer(text, return_tensors='pt')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "Hq4_zkSgpvKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores[2]"
      ],
      "metadata": {
        "id": "yqE6wjCxp0gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s= sentmm('''Curious_Indian_ But after selling we'll have to research again to buy new stocks, that is also time consuming''')"
      ],
      "metadata": {
        "id": "OJnEXnAbwNZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d5e7f3-ad61-400d-fad9-e84c2d544956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) negative 0.5477\n",
            "2) neutral 0.4279\n",
            "3) positive 0.0244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics"
      ],
      "metadata": {
        "id": "LtOYvGNPnXYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tweett(query,limit):\n",
        "  tweets = []\n",
        "  tweetscore = []\n",
        "\n",
        "  for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "      # print(tweet.content)\n",
        "      # break\n",
        "      if len(tweets) == limit:\n",
        "          break\n",
        "      else:\n",
        "          tweets.append([tweet.date, tweet.content])\n",
        "          x=sentmm(tweet.content)\n",
        "          tweetscore.append([x[0],x[1],x[2]])\n",
        "\n",
        "  sc = pd.DataFrame(tweetscore, columns=['-ve', '*','+ve'])\n",
        "  df = pd.DataFrame(tweets, columns=['Date', 'Tweet'])\n",
        "  # print(df)\n",
        "  # mn=statistics.mean(tweetscore)\n",
        "\n",
        "  return sc\n"
      ],
      "metadata": {
        "id": "-6CQhxucmEWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yjC2S6hvb1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=tweett('indian stocks today',50)\n",
        "zzz.append([x[\"-ve\"].mean(),x[\"*\"].mean(),x[\"+ve\"].mean()])\n",
        "zzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwH-zgsWlRkS",
        "outputId": "e63b7e72-8695-4266-9a5f-9ae74e51d777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.43886054, 0.46473092, 0.096408516], [0.06496478, 0.7541483, 0.18088686]]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=tweett('indian politics',50)\n",
        "zzz.append([x[\"-ve\"].mean(),x[\"*\"].mean(),x[\"+ve\"].mean()])\n",
        "zzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzUx2aQZtaAE",
        "outputId": "81dde3e7-2968-437c-d980-f6aea323e630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.43886054, 0.46473092, 0.096408516]]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}